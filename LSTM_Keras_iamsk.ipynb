{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0_LvL0ahEcS"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikit-learn==0.24.2\n",
        "# !pip install sklearn_crfsuite\n",
        "# try:\n",
        "#     from sklearn_crfsuite.metrics import flat_classification_report\n",
        "# except ImportError:\n",
        "#     !pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "#     from sklearn_crfsuite.metrics import flat_classification_report\n"
      ],
      "metadata": {
        "id": "j0CPD1pVcWio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y23RUjjjhEcS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "def read_file(f):\n",
        "    data = open(f,'r').readlines()[1:]\n",
        "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
        "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
        "    return row_id,data\n",
        "\n",
        "def read_conll_file(_file):\n",
        "    all_sentences = []\n",
        "    all_sentence_ids = [] \n",
        "    all_labels = []\n",
        "    sentence = []\n",
        "    labels = []\n",
        "    for line in tqdm(open(_file), desc=f\"reading {_file}\"):\n",
        "        if line.startswith(\"#\"):\n",
        "            all_sentence_ids.append(re.split(\"\\\\s+\", line.strip())[1])\n",
        "            continue\n",
        "        if not line.strip():\n",
        "            all_sentences.append(sentence)\n",
        "            all_labels.append(labels)\n",
        "            sentence = []\n",
        "            labels = []\n",
        "        else:\n",
        "            line = line.strip()\n",
        "            sentence.append(re.split(\"\\\\s+\", line)[0])\n",
        "            labels.append(re.split(\"\\\\s+\", line)[1])\n",
        "    if sentence and labels:\n",
        "        all_sentences.append(sentence)\n",
        "        all_labels.append(labels)\n",
        "    return all_sentence_ids, all_sentences, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcErfEePhEcT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"review_data\"):\n",
        "    !wget https://www.dropbox.com/s/yqgff7de73iwosr/review_data.zip?dl=1 -O review_data.zip\n",
        "    !unzip review_data.zip\n",
        "    !ls review_data \n",
        "\n",
        "train_sen_ids, train_text, train_labels = read_conll_file(\"review_data/review_train.conll\")\n",
        "valid_sen_ids, valid_text, valid_labels = read_conll_file(\"review_data/review_valid.conll\")\n",
        "test_sen_ids, test_text, test_labels = read_conll_file(\"review_data/review_test.conll\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHSh_Y8khEcT"
      },
      "source": [
        "### Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzjGFTxlhEcT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "unique_words = set([j for i in train_text + valid_text + test_text for j in i])\n",
        "word2idx = {j:i+1 for i,j in enumerate(unique_words)}\n",
        "word2idx[\"PAD\"] = 0\n",
        "print(f\"{len(word2idx)} tokens in vocab\")\n",
        "\n",
        "unique_labels = set([j for i in train_labels for j in i])\n",
        "unique_labels_valid = set([j for i in train_labels for j in i])\n",
        "unique_labels_test = set([j for i in train_labels for j in i])\n",
        "\n",
        "# make sure there are no labels in valid/test that are not in train.\n",
        "assert not unique_labels_valid - unique_labels, unique_labels_valid - unique_labels\n",
        "assert not unique_labels_test - unique_labels, unique_labels_test - unique_labels\n",
        "\n",
        "label2idx = {'PAD': 0}\n",
        "for i,j in enumerate(unique_labels):\n",
        "    label2idx[j] = i+1 \n",
        "idx2label = {j:i for i,j in label2idx.items()}\n",
        "print(idx2label)\n",
        "\n",
        "MAXLEN = 50\n",
        "\n",
        "def get_padded_x_y(text, labels, _maxlen, _word2idx, _label2idx):\n",
        "    X = [[word2idx[j] for j in i] for i in text]\n",
        "    X = pad_sequences(maxlen = _maxlen, sequences = X, padding = \"post\", value = _word2idx[\"PAD\"])\n",
        "    Y = [[label2idx[j] for j in i] for i in labels]\n",
        "    Y = pad_sequences(maxlen = _maxlen, sequences = Y, padding = \"post\", value = _label2idx[\"PAD\"])\n",
        "    Y = [to_categorical(i, num_classes = len(label2idx)) for i in Y]\n",
        "    assert len(X) == len(Y), \"X and Y should be of the same shape\"\n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = get_padded_x_y(train_text, train_labels, _maxlen=MAXLEN, _word2idx=word2idx, _label2idx=label2idx)\n",
        "X_valid, Y_valid = get_padded_x_y(valid_text, valid_labels, _maxlen=MAXLEN, _word2idx=word2idx, _label2idx=label2idx)\n",
        "X_test, Y_test = get_padded_x_y(test_text, test_labels, _maxlen=MAXLEN, _word2idx=word2idx, _label2idx=label2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH941rgChEcU"
      },
      "source": [
        "### LSTM model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOfXKFO9hEcU"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "# from tensorflow.keras.models import Sequential\n",
        "\n",
        "# model = Sequential()\n",
        "# EMBED_DIM = 300\n",
        "# RNN_HIDDEN_DIM = 100\n",
        "# model.add(Embedding(input_dim=len(word2idx.keys()),output_dim=EMBED_DIM,input_length=MAXLEN))\n",
        "# model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.2), merge_mode = 'concat'))\n",
        "# model.add(Dense(len(label2idx.keys()), activation=\"relu\"))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "# # model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQQ7IvqLhEcU"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hiMscQNhEcU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# # history = model.fit(X_train,np.array(Y_train),batch_size=16,epochs=3,validation_data=(X_valid, np.array(Y_valid)))\n",
        "# history = model.fit(X_train,np.array(Y_train),batch_size=16,epochs=10,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUfBsJ5JhEcU"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsqsBpIYhEcU"
      },
      "outputs": [],
      "source": [
        "# Y_valid_pred = model.predict(X_valid)\n",
        "# Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
        "# Y_valid_true = np.argmax(Y_valid, -1)\n",
        "# Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
        "# Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3N8ZhehEcU"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWQ9cfZGhEcU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObtpHfaGhEcU"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     from baseline.utils import to_chunks\n",
        "# except ImportError:\n",
        "#     !pip install mead-baseline\n",
        "#     from baseline.utils import to_chunks\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "# batchsz = 16\n",
        "\n",
        "# def shorten_sentence_label(sentence_tokens, true_labels, pred_labels, maxlen):\n",
        "#     if maxlen == -1: # we need to shorten the labels to the sentence length\n",
        "#         shorten_to = len(sentence_tokens)\n",
        "#     else: # we have to shorten either the sentence to the max len or the sequence to the sentence length\n",
        "#         if len(sentence_tokens) > maxlen:\n",
        "#             shorten_to = maxlen\n",
        "#         else:\n",
        "#             shorten_to = len(sentence_tokens)\n",
        "#     return sentence_tokens[:shorten_to], true_labels[:shorten_to], pred_labels[:shorten_to]\n",
        "\n",
        "\n",
        "\n",
        "# def generate_conll(sentence_ids,all_sentence_tokens, all_sentence_true_labels, all_sentence_pred_labels, output_base, \n",
        "#                    maxlen=-1):\n",
        "#     assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_true_labels) == len(all_sentence_pred_labels)\n",
        "#     with open(f\"{output_base}.conll\", \"w\") as wf:\n",
        "#         for sentence_tokens, sentence_true_labels, sentence_pred_labels in zip(all_sentence_tokens, all_sentence_true_labels, all_sentence_pred_labels):\n",
        "#             sentence_tokens, sentence_true_labels, sentence_pred_labels = shorten_sentence_label(\n",
        "#                 sentence_tokens, sentence_true_labels, sentence_pred_labels, maxlen)\n",
        "#             assert len(sentence_tokens) == len(sentence_true_labels) == len(sentence_pred_labels), \\\n",
        "#             f\"{len(sentence_tokens)}, {len(sentence_true_labels)}, {len(sentence_pred_labels)}\"\n",
        "#             for token, true_label, pred_label in zip(sentence_tokens, sentence_true_labels, sentence_pred_labels):\n",
        "#                 wf.write(f\"{token} {true_label} {pred_label}\\n\")\n",
        "#                 wf.write(\"\\n\")\n",
        "#     print(f\"generated conll file {output_base}.conll\")\n",
        "\n",
        "# def generate_labelseq(sentence_ids, all_sentence_tokens, all_sentence_pred_labels, output_base, maxlen=-1):\n",
        "#     assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_pred_labels)\n",
        "#     with open(f\"{output_base}.labelseq\", \"w\") as wf:\n",
        "#         wf.write(\"ID\\tSENTENCE\\tTAGSEQ\\n\")\n",
        "#         for sentence_id, sentence_tokens, sentence_labels in zip(sentence_ids, all_sentence_tokens, all_sentence_pred_labels):\n",
        "#             sentence_tokens, _, sentence_labels = shorten_sentence_label(sentence_tokens, sentence_labels, sentence_labels, maxlen)\n",
        "#             assert len(sentence_tokens) == len(sentence_labels)\n",
        "#             wf.write(f'\"{sentence_id}\"\\t\"{\" \".join(sentence_tokens)}\"\\t\"{\" \".join(sentence_labels)}\"\\n')\n",
        "#         print(f\"generated labelseq file {output_base}.labelseq\")\n",
        "\n",
        "# def generate_human_readable(sentence_ids, all_sentence_tokens, all_sentence_pred_labels, output_base, maxlen=-1):\n",
        "#     def create_chunk(tokens, chunk_def):\n",
        "#             chunk_type, chunk_indices = chunk_def.split(\"@\")[0], [int(x) for x in chunk_def.split(\"@\")[1:]]\n",
        "#             chunk_indices = chunk_indices + [chunk_indices[-1]+1]\n",
        "#             return f\"{chunk_type}: {' '.join(tokens[chunk_indices[0]: chunk_indices[-1]])}\"\n",
        "\n",
        "#     assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_pred_labels)\n",
        "#     with open(f\"{output_base}.human\", \"w\") as wf:\n",
        "#         for sentence_id, sentence_tokens, sentence_labels in zip(sentence_ids, all_sentence_tokens, all_sentence_pred_labels):\n",
        "#             wf.write(f\"[id]: {sentence_id}\\n\")\n",
        "#             sentence_tokens, _, sentence_labels = shorten_sentence_label(sentence_tokens, sentence_labels, sentence_labels, maxlen)\n",
        "#             assert len(sentence_tokens) == len(sentence_labels)\n",
        "#             wf.write(f\"[sentence]: {' '.join(sentence_tokens)}\\n\")\n",
        "#             chunks = to_chunks(sentence_labels, span_type=\"iob\") \n",
        "#             for chunk in chunks:\n",
        "#                 wf.write(create_chunk(sentence_tokens, chunk)+\"\\n\")\n",
        "#             wf.write(\"\\n\")\n",
        "#         print(f\"generated labelseq file {output_base}.human\")\n",
        "\n",
        "\n",
        "# def predict_tags_for_file(_file, model, _word2idx, _label2idx, output_base, output_formats=[\"human_readable\", \"labelseq\"]):\n",
        "#     sentence_ids, sen_texts, sen_labels = read_conll_file(_file)\n",
        "#     X, Y = get_padded_x_y(sen_texts, sen_labels, _maxlen=MAXLEN, _word2idx=_word2idx, _label2idx=_label2idx)\n",
        "#     Y_pred = np.argmax(model.predict(X), axis=-1)\n",
        "#     Y_true = np.argmax(Y, -1)\n",
        "#     Y_pred_labels = [[idx2label[i] for i in row] for row in Y_pred]\n",
        "#     Y_true_labels = [[idx2label[i] for i in row] for row in Y_true]\n",
        "#     if \"conll\" in output_formats:\n",
        "#         generate_conll(\n",
        "#             sentence_ids=sentence_ids,\n",
        "#             all_sentence_tokens=sen_texts, \n",
        "#             all_sentence_true_labels=Y_true_labels, \n",
        "#             all_sentence_pred_labels=Y_pred_labels, \n",
        "#             output_base=output_base,\n",
        "#             maxlen=MAXLEN\n",
        "#         )\n",
        "#     if \"labelseq\" in output_formats:\n",
        "#         generate_labelseq(\n",
        "#             sentence_ids=sentence_ids,\n",
        "#             all_sentence_tokens=sen_texts, \n",
        "#             all_sentence_pred_labels=Y_pred_labels, \n",
        "#             output_base=output_base,\n",
        "#             maxlen=MAXLEN\n",
        "#         )\n",
        "#     if \"human_readable\" in output_formats:\n",
        "#          generate_human_readable(\n",
        "#             sentence_ids=sentence_ids,\n",
        "#             all_sentence_tokens=sen_texts, \n",
        "#             all_sentence_pred_labels=Y_pred_labels, \n",
        "#             output_base=output_base,\n",
        "#             maxlen=MAXLEN\n",
        "#         )\n",
        "# # let's first run this method on the validation data because we have the true labels available for it.\n",
        "# test_file=\"review_data/review_valid.conll\"\n",
        "# predict_tags_for_file(test_file, model=model, _label2idx=label2idx, _word2idx=word2idx, output_base=\"valid_output\", \n",
        "#                       output_formats=[\"human_readable\", \"labelseq\", \"conll\"])\n",
        "# # and evaluate with conll eval\n",
        "# if not os.path.exists(\"conlleval.pl\"):\n",
        "#     !wget https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt -O conlleval.pl\n",
        "# !perl ./conlleval.pl < valid_output.conll\n",
        "\n",
        "# # finally, run this method on the test data and look at the generated labelseq file.\n",
        "# test_file=\"review_data/review_test.conll\"\n",
        "# predict_tags_for_file(test_file, model=model, _label2idx=label2idx, _word2idx=word2idx, output_base=\"test_output\", \n",
        "#                       output_formats=[\"human_readable\", \"labelseq\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # Define hyperparameters\n",
        "# EMBED_DIM = 300\n",
        "# RNN_HIDDEN_DIM = 100\n",
        "# DROPOUT_RATE = 0.2\n",
        "# LEARNING_RATE = 0.001\n",
        "# BATCH_SIZE = 32\n",
        "# EPOCHS = 10\n",
        "\n",
        "# # Create the LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=len(word2idx.keys()), output_dim=EMBED_DIM, input_length=MAXLEN))\n",
        "# model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM, return_sequences=True, dropout=DROPOUT_RATE), merge_mode='concat'))\n",
        "# model.add(Dense(len(label2idx.keys()), activation=\"relu\"))\n",
        "\n",
        "# # Compile the model with chosen optimizer and learning rate\n",
        "# optimizer = Adam(lr=LEARNING_RATE)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])"
      ],
      "metadata": {
        "id": "K1IfUEYAGZ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# # history = model.fit(X_train,np.array(Y_train),batch_size=16,epochs=3,validation_data=(X_valid, np.array(Y_valid)))\n",
        "# history = model.fit(X_train,np.array(Y_train),batch_size=16,epochs=10,validation_split=0.2)\n",
        "# Y_valid_pred = model.predict(X_valid)\n",
        "# Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
        "# Y_valid_true = np.argmax(Y_valid, -1)\n",
        "# Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
        "# Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]\n",
        "# try:\n",
        "#     from sklearn_crfsuite.metrics import flat_classification_report\n",
        "# except ImportError:\n",
        "#     !pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "#     from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "# report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels)\n",
        "# print(report)"
      ],
      "metadata": {
        "id": "fdDqRuLBGcV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # Define hyperparameters\n",
        "# EMBED_DIM = 300\n",
        "# RNN_HIDDEN_DIM = 100\n",
        "# DROPOUT_RATE = 0.5  # increased dropout rate\n",
        "# LEARNING_RATE = 0.001\n",
        "# BATCH_SIZE = 32\n",
        "# EPOCHS = 20  # increased number of epochs\n",
        "\n",
        "# # Create the LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=len(word2idx.keys()), output_dim=EMBED_DIM, input_length=MAXLEN))\n",
        "# model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM, return_sequences=True, dropout=DROPOUT_RATE), merge_mode='concat'))\n",
        "# model.add(Dense(len(label2idx.keys()), activation=\"relu\"))\n",
        "# model.add(Dropout(DROPOUT_RATE))  # added dropout layer after Dense layer\n",
        "\n",
        "# # Compile the model with chosen optimizer and learning rate\n",
        "# optimizer = Adam(lr=LEARNING_RATE)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "\n",
        "# # Train the model with updated hyperparameters\n",
        "# history = model.fit(X_train, np.array(Y_train), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)\n",
        "\n",
        "# # Evaluate the model\n",
        "# Y_valid_pred = model.predict(X_valid)\n",
        "# Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
        "# Y_valid_true = np.argmax(Y_valid, -1)\n",
        "# Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
        "# Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]\n",
        "\n",
        "# # Calculate classification report\n",
        "# report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels)\n",
        "# print(report)\n"
      ],
      "metadata": {
        "id": "7taIDwb_HYoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.optimizers import Adagrad\n",
        "\n",
        "# # Define hyperparameters\n",
        "# EMBED_DIM = 300\n",
        "# RNN_HIDDEN_DIM = 100\n",
        "# DROPOUT_RATE = 0.2\n",
        "# LEARNING_RATE = 0.01 # Adagrad specific learning rate\n",
        "# BATCH_SIZE = 16\n",
        "# EPOCHS = 10\n",
        "\n",
        "# # Create the LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=len(word2idx.keys()), output_dim=EMBED_DIM, input_length=MAXLEN))\n",
        "# model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM, return_sequences=True, dropout=DROPOUT_RATE), merge_mode='concat'))\n",
        "# model.add(Dense(len(label2idx.keys()), activation=\"relu\"))\n",
        "\n",
        "# # Compile the model with Adagrad optimizer and learning rate\n",
        "# optimizer = Adagrad(lr=LEARNING_RATE)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(X_train, np.array(Y_train), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)\n",
        "\n",
        "# # Make predictions and evaluate the model\n",
        "# Y_valid_pred = model.predict(X_valid)\n",
        "# Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
        "# Y_valid_true = np.argmax(Y_valid, -1)\n",
        "# Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
        "# Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]\n",
        "\n",
        "# # Print classification report\n",
        "# try:\n",
        "#     from sklearn_crfsuite.metrics import flat_classification_report\n",
        "# except ImportError:\n",
        "#     !pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "#     from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "# report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels)\n",
        "# print(report)\n"
      ],
      "metadata": {
        "id": "uhTJdnPPHvVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINAL LSTM MODEL\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "EMBED_DIM = 30\n",
        "RNN_HIDDEN_DIM = 50 #reduce\n",
        "DROPOUT_RATE = 0.2\n",
        "LEARNING_RATE = 0.01  # Set the learning rate for SGD optimizer\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word2idx.keys()), output_dim=EMBED_DIM, input_length=MAXLEN))\n",
        "model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM, return_sequences=True, dropout=DROPOUT_RATE, ), merge_mode='concat'))\n",
        "model.add(Dense(len(label2idx.keys()), activation=\"relu\"))\n",
        "\n",
        "# Compile the model with SGD optimizer and learning rate\n",
        "#optimizer = SGD(lr=LEARNING_RATE)  # Use SGD optimizer with specified learning rate\n",
        "#adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "# configure the optimizer\n",
        "optimizer = Adam()\n",
        "#optimizer = Adagrad(learning_rate=LEARNING_RATE, epsilon=1e-10)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, np.array(Y_train), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "Y_valid_pred = model.predict(X_valid)\n",
        "Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
        "Y_valid_true = np.argmax(Y_valid, -1)\n",
        "Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
        "Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]\n",
        "\n",
        "# Generate classification report\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "51hJZ2DEIQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "# print(confusion_matrix(y_true=Y_valid_true_labels, y_pred=Y_valid_pred_labels))"
      ],
      "metadata": {
        "id": "LQhNCekm3Aas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST DATA\n",
        "\n",
        "# Evaluate the model\n",
        "Y_test_pred = model.predict(X_test)\n",
        "#Y_test_pred = np.argmax(Y_test_pred, axis=-1)\n",
        "Y_test_true = np.argmax(Y_test, -1)\n",
        "#Y_test_pred_labels = [[idx2label[i] for i in row] for row in Y_test_pred]\n",
        "Y_test_true_labels = [[idx2label[i] for i in row] for row in Y_test_true]\n"
      ],
      "metadata": {
        "id": "e7d066X43A51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#file1 = pd.read_csv(\"TEST_REVIEW_TEXT.txt\", sep='\\t')\n",
        "test_sen_ids"
      ],
      "metadata": {
        "id": "tGhSP4HEHjmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Y_test_true_labels, all labels are \"O\" on the test data probably due to overfitting of the model. \n",
        "#Therefore, the model chosen was CRF with 89% accuracy. The LSTM model has 96% accuracy."
      ],
      "metadata": {
        "id": "jDcA3GdRf4kr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}